# Домашнее задание к занятию «Микросервисы: подходы»

https://github.com/netology-code/micros-homeworks/blob/main/11-microservices-03-approaches.md


Вы работаете в крупной компании, которая строит систему на основе микросервисной архитектуры.
Вам как DevOps-специалисту необходимо выдвинуть предложение по организации инфраструктуры для разработки и эксплуатации.


## Задача 1: Обеспечить разработку

Предложите решение для обеспечения процесса разработки: хранение исходного кода, непрерывная интеграция и непрерывная поставка. 
Решение может состоять из одного или нескольких программных продуктов и должно описывать способы и принципы их взаимодействия.

Решение должно соответствовать следующим требованиям:
- облачная система;
- система контроля версий Git;
- репозиторий на каждый сервис;
- запуск сборки по событию из системы контроля версий;
- запуск сборки по кнопке с указанием параметров;
- возможность привязать настройки к каждой сборке;
- возможность создания шаблонов для различных конфигураций сборок;
- возможность безопасного хранения секретных данных (пароли, ключи доступа);
- несколько конфигураций для сборки из одного репозитория;
- кастомные шаги при сборке;
- собственные докер-образы для сборки проектов;
- возможность развернуть агентов сборки на собственных серверах;
- возможность параллельного запуска нескольких сборок;
- возможность параллельного запуска тестов.

Обоснуйте свой выбор.

## Задача 2: Логи

Предложите решение для обеспечения сбора и анализа логов сервисов в микросервисной архитектуре.
Решение может состоять из одного или нескольких программных продуктов и должно описывать способы и принципы их взаимодействия.

Решение должно соответствовать следующим требованиям:
- сбор логов в центральное хранилище со всех хостов, обслуживающих систему;
- минимальные требования к приложениям, сбор логов из stdout;
- гарантированная доставка логов до центрального хранилища;
- обеспечение поиска и фильтрации по записям логов;
- обеспечение пользовательского интерфейса с возможностью предоставления доступа разработчикам для поиска по записям логов;
- возможность дать ссылку на сохранённый поиск по записям логов.

Обоснуйте свой выбор.

## Задача 3: Мониторинг

Предложите решение для обеспечения сбора и анализа состояния хостов и сервисов в микросервисной архитектуре.
Решение может состоять из одного или нескольких программных продуктов и должно описывать способы и принципы их взаимодействия.

Решение должно соответствовать следующим требованиям:
- сбор метрик со всех хостов, обслуживающих систему;
- сбор метрик состояния ресурсов хостов: CPU, RAM, HDD, Network;
- сбор метрик потребляемых ресурсов для каждого сервиса: CPU, RAM, HDD, Network;
- сбор метрик, специфичных для каждого сервиса;
- пользовательский интерфейс с возможностью делать запросы и агрегировать информацию;
- пользовательский интерфейс с возможностью настраивать различные панели для отслеживания состояния системы.

Обоснуйте свой выбор.

## Задача 4: Логи * (необязательная)

Продолжить работу по задаче API Gateway https://github.com/Spardoks/Microservices_principles: сервисы, используемые в задаче, пишут логи в stdout. 

Добавить в систему сервисы для сбора логов Vector + ElasticSearch + Kibana со всех сервисов, обеспечивающих работу API.

### Результат выполнения: 

docker compose файл, запустив который можно перейти по адресу http://localhost:8081, по которому доступна Kibana.
Логин в Kibana должен быть admin, пароль qwerty123456.


## Задача 5: Мониторинг * (необязательная)

Продолжить работу по задаче API Gateway https://github.com/Spardoks/Microservices_principles: сервисы, используемые в задаче, предоставляют набор метрик в формате prometheus:

- сервис security по адресу /metrics,
- сервис uploader по адресу /metrics,
- сервис storage (minio) по адресу /minio/v2/metrics/cluster.

Добавить в систему сервисы для сбора метрик (Prometheus и Grafana) со всех сервисов, обеспечивающих работу API.
Построить в Graphana dashboard, показывающий распределение запросов по сервисам.

### Результат выполнения: 

docker compose файл, запустив который можно перейти по адресу http://localhost:8081, по которому доступна Grafana с настроенным Dashboard.
Логин в Grafana должен быть admin, пароль qwerty123456.

---

## Решение 1 - Обеспечение разработки

Для проекта, где каждый микросервис хранится в отдельном репозитории, а процесс разработки должен быть полностью автоматизирован — от push‑а кода до поставки в продакшн — наиболее удобным мне показался GitLab + само‑хостимые GitLab‑раннеры.

1. Облачная система - GitLab.com – полностью управляемый SaaS‑сервис.
2. Git‑контроль версий - Встроенный Git‑сервер, поддержка LFS, Web‑IDE.
3. Репозиторий на каждый сервис - Каждый микросервис – отдельный Project в GitLab; можно группировать их в Group (например, company/microservices).
4. Запуск сборки по событию из VCS - pipeline автоматически создаётся при push, merge_request, tag.
5. Запуск сборки по кнопке с указанием параметров - manual‑jobs (button) + pipeline variables в UI; также API‑trigger (POST /projects/:id/trigger/pipeline).
6. Привязка настроек к каждой сборке - CI/CD > Variables – protected, masked, file; задаются на уровне проекта, группы, среды, ветки; также можно объявлять job‑level variables.
7. Шаблоны для различных конфигураций сборок - include: → импорт из другого репозитория; extends: → наследование; rules: → условный запуск.
8. Безопасное хранение секретных данных - Masked / Protected variables, Secret files, интеграция с HashiCorp Vault (GitLab 13+).
9. Несколько конфигураций сборки из одного репозитория - rules:/only:/except: + dynamic child pipelines; можно генерировать матрицу (parallel:matrix).
10. Кастомные шаги при сборке - Любой bash‑скрипт в script:, before_script, after_script; также docker-in-docker для сложных сценариев.
11. Собственные Docker‑образы для сборки - image: my-registry.com/custom-builder:latest; образ можно собрать в отдельном stage: build-image.
12. Развёртывание агентов сборки на собственных серверах - GitLab Runner устанавливается на любой сервер (Linux, Windows, macOS) или в Kubernetes (helm/gitlab-runner). Можно настроить tags → job‑ы привязываются к нужным раннерам.
13. Параллельный запуск нескольких сборок - При наличии нескольких раннеров CI автоматически распределяет jobs; можно ограничить/разрешить параллелизм через resource_group.
14. Параллельный запуск тестов - parallel: N в job‑е, либо parallel:matrix для разных параметров (браузер, версия API). Тест‑фреймворки (JUnit, pytest‑xdist, Go test –parallel) делят тест‑suite внутри контейнера.

## Решение 2 - Логи

С учётом указанных требований решение может содержать следующие компоненты

| Компонент | Задача | Обоснование |
|:---:|:---:|:---:|
|Fluent Bit (или Fluent D) – агент‑коллектор, развёрнутый как DaemonSet на каждом узле|Чтение stdout/stderr контейнеров, парсинг, буферизация, отправка в Kafka|Лёгкий (≈ 5 МБ), работает без изменения кода приложений, умеет гарантировать доставку (встроенный file‑buffer и retries).|
|Apache Kafka (топик logs) – распределённый брокер‑очередь|Надёжный, «at‑least‑once» транспорт между агентом и системой индексации, возможность репликации и горизонтального масштабирования|Гарантированная доставка, сохранность сообщений при сбоях, возможность «откатывать» назад и «re‑process» при обновлении парсеров.|
|Logstash (или Kafka Connect + Elasticsearch Sink) – процессор сообщений|Преобразование (парсинг JSON/Logfmt/Regex), обогащение (metadata‑host, service‑name, k8s‑labels) и отправка в Elasticsearch|Полноценный pipeline, поддержка схемы, возможность динамического изменения без перезапуска агентов.|
|Elasticsearch – центральное хранилище и поисковый движок|Индексация, полнотекстовый поиск, агрегации, хранение ≥ 90 дней (можно подключить S3‑cold‑storage)|Высокая производительность, масштабируемость, поддержка сложных запросов, готовый REST‑API.|
|Kibana – UI‑консоль|Поиск, фильтрация, построение дашбордов, сохранённые запросы, шаринг ссылки|Интуитивный UI, RBAC (Space‑based), возможность делиться «Saved Search» через URL‑параметры.|
|Grafana + Loki (опционально)|Быстрый просмотр “raw‑log” в виде потоков, если нужны «tail‑like» запросы|Лёгкая альтернатива для разработчиков, использует тот же Kafka‑источник через Promtail.|
|Vault / K8s Secrets|Хранение учетных данных к Kafka, Elasticsearch|Безопасный доступ к паролям/токенам, интеграция с RBAC‑политиками.|

## Решение 3 - Мониторинг

С учётом указанных требований решение может содержать следующие компоненты, в центре которых Prometheus + Grafana.

1. Prometheus – центральный сервер, «pull‑модель» (скрейпинг) и многомерная модель метрик.
2. node‑exporter – собирает метрики хоста (CPU, RAM, диск, сеть, файловая система).
3. Prometheus‑client‑library (Go, Java, Python, .NET, …) – в каждом микросервисе публикует свои бизнес‑ и сервис‑специфичные метрики (latency, request‑count, queue‑size, cache‑hits, …).
4. Grafana – UI для запросов, агрегаций и построения настраиваемых панелей (dashboards).
5. Alertmanager – (опционально) обработка алертов и их доставка.
6. Thanos / Cortex – (опционально) долгосрочное хранилище и HA‑репликация Prometheus.

Можно также рассмотреть InfluxDB + Telegraf, Zabbix / Nagios
